---
---

@unpublished{huerta2024prompt,
  title={Prompt Weight Experiments for LLM Instruction Fine-Tuning},
  author={Huerta-Enochian, Mathew},
  institution={EQ4ALL},
  abbr={Preprint},
  booktitle={Preprint},
  abstract={We present a small study analyzing how prompt token classification loss weighting (PLW) affects the performance of 7B-size LLaMA models fine-tuned on instruction tasks. We recreated Stanford's Alpaca experiment with both LLaMA 1 and LLaMA 2 using multiple instruction datasets. We found that models fine-tuned on our short-completion dataset have a negative quadratic relationship with PLW while models fine-tuned on long-completion datasets were unaffected by PLW.},
  arxiv={2401.13586},
  year={2024}
}

@unpublished{kim2024signbleu,
  title={SignBLEU: Automatic Evaluation of Multi-Channel Sign Language Translation},
  author={Kim*, Jung-Ho and Huerta-Enochian*, Mathew and Ko, Changyong and Lee, Du Hui},
  institution={EQ4ALL},
  abbr={LREC-COLING},
  abstract={Sign language is a multi-channel language, communicating information through not just the hands (manual signals) but also facial expressions and upper body movements (non-manual signals). However, since automatic sign language translation is usually performed by generating a single sequence of glosses, researchers eschew non-manual and co-occurring manual signals in favor of a simplified list of manual glosses. This can lead to significant information loss and ambiguity. In this paper, we formally introduce a new task named multi-channel sign language translation (MCSLT) and present a novel metric, SignBLEU, designed to capture multiple signal channels. We validated SignBLEU on a system-level task using three sign language corpora with varied linguistic structures and transcription methodologies and examined its correlation with human judgment through two segment-level tasks. We found that SignBLEU consistently correlates better with human judgment than competing metrics. To facilitate further MCSLT research, we report benchmark scores for the three sign language corpora and release the source code for SignBLEU.},
  year={2024},
  organization={LC},
  note={Accepted}
}

@inproceedings{kim2023astudy,
  title={A Study on Korean to Korean Sign Language Translation using Large Language Models},
  author={Kim, Jung-Ho and Ko, Changyong and Huerta-Enochian, Mathew and Lee, Jongcheon and Lee, Du Hui},
  booktitle={Korea Artificial Intelligence Conference},
  year={2023}
}

@inproceedings{huerta2023sign,
  title={Sign Language Avatar Animation Search: An Ani2Ani Search Application},
  author={Huerta-Enochian, Mathew and Ko, Changyong},
  abstract={To improve sign language animation asset management, we developed a general animation search system supporting multiple input modalities. The system reads animations as pose sequences, embeds them into fixed-length representation vectors using engineered or learned features, and scores animation similarity using distance in the embedding space. We present an overview of the system, use-case scenarios, and feedback from deaf users.},
  abbr={SLTAT},
  booktitle={2023 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)},
  pages={1--5},
  year={2023},
  organization={IEEE},
  poster={huerta2023sign_poster.pdf}
}

@inproceedings{huerta2022kosign,
  title={KoSign Sign Language Translation Project: Introducing the NIASL2021 Dataset},
  author={Huerta-Enochian, Mathew and Lee, Du Hui and Myung, Hye Jin and Byun, Kang Suk and Lee, Jun Woo},
  abstract={We introduce a new sign language production (SLP) and sign language translation (SLT) dataset, NIASL2021, consisting of 201,026 Korean-KSL data pairs. KSL translations of Korean source texts are represented in three formats: video recordings, keypoint position data, and time-aligned gloss annotations for each hand (using a 7,989 sign vocabulary) and for eight different non-manual signals (NMS). We evaluated our sign language elicitation methodology and found that text-based prompting had a negative effect on translation quality in terms of naturalness and comprehension. We recommend distilling text into a visual medium before translating into sign language or adding a prompt-blind review step to text-based translation methodologies},
  abbr={SLTAT},
  booktitle={Proceedings of the 7th International Workshop on Sign Language Translation and Avatar Technology: The Junction of the Visual and the Textual: Challenges and Perspectives},
  pages={59--66},
  year={2022},
  poster={huerta2022kosign_poster.pdf}
}
