---
---

@unpublished{huerta2024prompt,
  title={Instruction Fine-Tuning: Does Prompt Loss Matter?},
  author={Huerta-Enochian, Mathew and Ko, Seung Yong},
  institution={EQ4ALL},
  abbr={EMNLP},
  booktitle={The 2024 Conference on Empirical Methods in Natural Language Processing},
  abstract={We present a novel study analyzing the effects of various prompt loss token weights (PLW) for supervised instruction fine-tuning (SIFT). While prompt-masking (PLW = 0) is common for SIFT, some fine-tuning APIs support fractional PLWs and suggest that using a small non-zero PLW can help stabilize learning when fine-tuning on short-completion data. However, there has never been a study confirming this claim, and OpenAI, a major cloud-based SIFT provider, recently removed this parameter from their fine-tuning API. We found that performance of models fine-tuned on short-completion data had a statistically-significant negative quadratic relationship with PLW. Using small values (0.01 - 0.5) of PLW produced better results on multiple-choice and short-generation benchmarks (outperforming models fine-tuned on long-completion data) while large values (~ 1.0) of PLW produced better results on long-generation benchmarks. We explained this effect and verified its importance through additional experiments. This research serves as a warning to API providers about the importance of providing a PLW parameter for SIFT.},
  arxiv={2401.13586},
  website={https://openreview.net/forum?id=XmWn8oVa6e},
  year={2024}
}

@inproceedings{kim2024shedding,
  title={Shedding Light on the Underexplored: Tackling the Minor Sign Language Research Topics},
  author={Kim, Jung-Ho and Ko, Changyong and Huerta-Enochian, Mathew and Ko, Seung Yong},
  abbr={SignLang},
  pages={240--251},
  booktitle={Proceedings of the {LREC-COLING} 2024 11th Workshop on the Representation and Processing of Sign Languages: Evaluation of Sign Language Resources},
  maintitle={2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation ({LREC-COLING} 2024)},
  abstract={In the past decade, sign language research has achieved remarkable results alongside the advancements in deep learning. However, there is a disconnect between the outcomes of these research efforts and the actual use of sign language by signers. In this position paper, we reviewed sign language papers related to deep learning published in the last ten years to explore reasons for this gap. We found many areas of research that are still underdeveloped, despite their linguistic importance. Based on an analysis of known corpora and methodologies, we identified the reasons for the lack of progress in these areas and propose directions for future research efforts.},
  publisher={{ELRA Language Resources Association (ELRA) and the International Committee on Computational Linguistics (ICCL)}},
  address={Torino, Italy},
  day={25},
  month={may},
  year={2024},
  isbn={978-2-493814-30-2},
  language={english},
  website={https://www.sign-lang.uni-hamburg.de/lrec/pub/24028.pdf}
}

@inproceedings{kim2024signbleu,
  title={{S}ign{BLEU}: Automatic Evaluation of Multi-channel Sign Language Translation},
  author={Kim*, Jung-Ho  and  Huerta-Enochian*, Mathew John  and  Ko, Changyong  and  Lee, Du Hui},
  abbr={LREC-COLING},
  year={2024},
  month={may},
  booktitle={Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  publisher={ELRA and ICCL},
  address={Torino, Italy},
  pages={14796--14811},
  website={https://aclanthology.org/2024.lrec-main.1289},
  abstract={Sign languages are multi-channel languages that communicate information through not just the hands (manual signals) but also facial expressions and upper body movements (non-manual signals). However, since automatic sign language translation is usually performed by generating a single sequence of glosses, researchers eschew non-manual and co-occurring manual signals in favor of a simplified list of manual glosses. This can lead to significant information loss and ambiguity. In this paper, we introduce a new task named multi-channel sign language translation (MCSLT) and present a novel metric, SignBLEU, designed to capture multiple signal channels. We validated SignBLEU on a system-level task using three sign language corpora with varied linguistic structures and transcription methodologies and examined its correlation with human judgment through two segment-level tasks. We found that SignBLEU consistently correlates better with human judgment than competing metrics. To facilitate further MCSLT research, we report benchmark scores for the three sign language corpora and release the source code for SignBLEU at https://github.com/eq4all-projects/SignBLEU.}
}

@inproceedings{kim2023astudy,
  title={A Study on Korean to Korean Sign Language Translation using Large Language Models},
  author={Kim, Jung-Ho and Ko, Changyong and Huerta-Enochian, Mathew and Lee, Jongcheon and Lee, Du Hui},
  abbr={KAIC},
  booktitle={Korea Artificial Intelligence Conference},
  year={2023}
}

@inproceedings{huerta2023sign,
  title={Sign Language Avatar Animation Search: An Ani2Ani Search Application},
  author={Huerta-Enochian, Mathew and Ko, Changyong},
  abstract={To improve sign language animation asset management, we developed a general animation search system supporting multiple input modalities. The system reads animations as pose sequences, embeds them into fixed-length representation vectors using engineered or learned features, and scores animation similarity using distance in the embedding space. We present an overview of the system, use-case scenarios, and feedback from deaf users.},
  abbr={SLTAT},
  booktitle={2023 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)},
  pages={1--5},
  year={2023},
  organization={IEEE},
  poster={huerta2023sign_poster.pdf}
}

@inproceedings{huerta2022kosign,
  title={KoSign Sign Language Translation Project: Introducing the NIASL2021 Dataset},
  author={Huerta-Enochian, Mathew and Lee, Du Hui and Myung, Hye Jin and Byun, Kang Suk and Lee, Jun Woo},
  abstract={We introduce a new sign language production (SLP) and sign language translation (SLT) dataset, NIASL2021, consisting of 201,026 Korean-KSL data pairs. KSL translations of Korean source texts are represented in three formats: video recordings, keypoint position data, and time-aligned gloss annotations for each hand (using a 7,989 sign vocabulary) and for eight different non-manual signals (NMS). We evaluated our sign language elicitation methodology and found that text-based prompting had a negative effect on translation quality in terms of naturalness and comprehension. We recommend distilling text into a visual medium before translating into sign language or adding a prompt-blind review step to text-based translation methodologies},
  abbr={SLTAT},
  booktitle={Proceedings of the 7th International Workshop on Sign Language Translation and Avatar Technology: The Junction of the Visual and the Textual: Challenges and Perspectives},
  pages={59--66},
  year={2022},
  website={https://aclanthology.org/2022.sltat-1.9/},
  poster={huerta2022kosign_poster.pdf}
}
