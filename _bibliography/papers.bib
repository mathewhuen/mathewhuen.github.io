---
---

@unpublished{huerta2024prompt,
  title={Prompt Weight Experiments for LLM Instruction Fine-Tuning},
  author={Huerta-Enochian, Mathew},
  institution={EQ4ALL},
  abbr={Preprint},
  booktitle={Preprint},
  abstract={We present a small study analyzing how prompt token classification loss weighting (PLW) affects the performance of 7B-size LLaMA models fine-tuned on instruction tasks. We recreated Stanford's Alpaca experiment with both LLaMA 1 and LLaMA 2 using multiple instruction datasets. We found that models fine-tuned on our short-completion dataset have a negative quadratic relationship with PLW while models fine-tuned on long-completion datasets were unaffected by PLW.},
  arxiv={2401.13586},
  pdf={2401.13586.pdf},
  year={2024}
}

@inproceedings{kim2023astudy,
  title={A Study on Korean to Korean Sign Language Translation using Large Language Models},
  author={Kim, Jung-Ho and Ko, Changyong and Huerta-Enochian, Mathew and Lee, Jongcheon and Lee, Du Hui},
  booktitle={Korea Artificial Intelligence Conference},
  year={2023}
}

@inproceedings{huerta2023sign,
  title={Sign Language Avatar Animation Search: An Ani2Ani Search Application},
  author={Huerta-Enochian, Mathew and Ko, Changyong},
  abstract={To improve sign language animation asset management, we developed a general animation search system supporting multiple input modalities. The system reads animations as pose sequences, embeds them into fixed-length representation vectors using engineered or learned features, and scores animation similarity using distance in the embedding space. We present an overview of the system, use-case scenarios, and feedback from deaf users.},
  booktitle={2023 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)},
  pages={1--5},
  year={2023},
  organization={IEEE},
  poster={huerta2023sign_poster.pdf}
}

@inproceedings{huerta2022kosign,
  title={KoSign Sign Language Translation Project: Introducing the NIASL2021 Dataset},
  author={Huerta-Enochian, Mathew and Lee, Du Hui and Myung, Hye Jin and Byun, Kang Suk and Lee, Jun Woo},
  abstract={We introduce a new sign language production (SLP) and sign language translation (SLT) dataset, NIASL2021, consisting of 201,026 Korean-KSL data pairs. KSL translations of Korean source texts are represented in three formats: video recordings, keypoint position data, and time-aligned gloss annotations for each hand (using a 7,989 sign vocabulary) and for eight different non-manual signals (NMS). We evaluated our sign language elicitation methodology and found that text-based prompting had a negative effect on translation quality in terms of naturalness and comprehension. We recommend distilling text into a visual medium before translating into sign language or adding a prompt-blind review step to text-based translation methodologies},
  booktitle={Proceedings of the 7th International Workshop on Sign Language Translation and Avatar Technology: The Junction of the Visual and the Textual: Challenges and Perspectives},
  pages={59--66},
  year={2022},
  poster={huerta2022kosign_poster.pdf}
}
